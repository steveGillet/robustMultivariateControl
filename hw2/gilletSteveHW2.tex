\documentclass[12pt, letterpaper]{article}

% Packages
\usepackage[margin=1in]{geometry} % For setting page margins
\usepackage{amsmath, amssymb} % For math symbols and equations
\usepackage{graphicx} % For including images
\usepackage{hyperref} % For hyperlinks

\renewcommand{\thesection}{\arabic{section}.}

\begin{document}

\title{
    \begin{tabular}{@{}l@{}}
        \textbf{Class:} Robust Multivariate Control \\
        \textbf{Professor:} Dr. Sean Humbert \\
        \textbf{TAs:} Santosh Chaganti \\
        \textbf{Student:} Steve Gillet \\
        \textbf{Date:} \today \\
        \textbf{Assignment:} Homework 2
    \end{tabular}
}

\author{}
\date{}

\maketitle

\section{Find the singular value decompositions (SVDs) of the following:}
\subsection*{(a) $A = \begin{bmatrix} 2 & -1 & 2 \end{bmatrix}$}

Find SVD
\begin{align*}
    A &= \begin{bmatrix} 2 & -1 & 2 \end{bmatrix} \\
    A^*A &= \begin{bmatrix} 2 \\ -1 \\ 2 \end{bmatrix}\begin{bmatrix} 2 & -1 & 2 \end{bmatrix} \\
    &= \begin{bmatrix} 4 & -2 & 4 \\ -2 & 1 & -2 \\ 4 & -2 & 4 \end{bmatrix} \\
    A^*A - \sigma^2 I &= \begin{bmatrix} 4 - \lambda & -2 & 4 \\ -2 & 1 - \lambda & -2 \\ 4 & -2 & 4 - \lambda \end{bmatrix} \\
    \det(A^*A - \sigma^2 I) &= 0 \\
    -\lambda^3 + 9 \lambda^2 &= 0 \\
    -\lambda^2(\lambda - 9) &= 0 \\
    \lambda_{2,3} = 0, \lambda_1 &= 9 \\
    (A - \lambda_{2,3} I) v &= 0
\end{align*}
\begin{align*}
    \begin{bmatrix} 4 & -2 & 4 \\ -2 & 1 & -2 \\ 4 & -2 & 4 \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} &= \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} \\
    x_{2,3} &= c_1\begin{bmatrix} 1 \\ 4 \\ 1 \end{bmatrix} \\
    &= \frac{1}{\sqrt{18}}\begin{bmatrix} 1 \\ 4 \\ 1 \end{bmatrix} \\
    \begin{bmatrix} -5 & -2 & 4 \\ -2 & -8 & -2 \\ 4 & -2 & -5 \end{bmatrix} x &= \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} \\
    x_1 &= \frac{1}{3} \begin{bmatrix} 2 \\ -1 \\ 2 \end{bmatrix} \\
    \sigma_1 &= 3, v_1 = \frac{1}{3} \begin{bmatrix} 2 \\ -1 \\ 2 \end{bmatrix} \\
    \sigma_2, \sigma_3 &= 0 \quad v_{2,3} = \frac{1}{\sqrt{18}} \begin{bmatrix} 1 \\ 4 \\ 1\end{bmatrix}\\
    u_1 &= \frac{A v_1}{\sigma_1} \\
    u_1 &= \frac{1}{3} \begin{bmatrix} 2 & -1 & 2 \end{bmatrix} \frac{1}{3} \begin{bmatrix} 2 \\ -1 \\ 2 \end{bmatrix} = \frac{1}{9} \cdot 9 \\
    u_{2,3} &= 0 \begin{bmatrix} 2 & -1 & 2 \end{bmatrix} \frac{1}{\sqrt{18}} \begin{bmatrix} 1 \\ 4 \\ 1 \end{bmatrix} = 0 \\
    A &= \left[ 1 \right] \begin{bmatrix} 3 & 0 & 0 \end{bmatrix} \begin{bmatrix} \frac{2}{3} & -\frac{1}{3} & \frac{2}{3} \\ \frac{1}{\sqrt{18}} & \frac{4}{\sqrt{18}} & \frac{1}{\sqrt{18}} \\ \frac{1}{\sqrt{18}} & \frac{4}{\sqrt{18}} & \frac{1}{\sqrt{18}} \end{bmatrix}
\end{align*}

\subsection*{(b) The $m \times n$ matrix of zeros, $O_{m \times n}$.}

For a $0$ matrix you are going to get $0$ for all the singular values because the determinant of that $A^*A - \lambda I$ is just going to give you n $\lambda = 0$. The $V^T$ matrix will be $n \times n$ with 0s in the first row and then the other rows any vectors orthonormal to the 0 vector, same with the U matrix columns except the U matrix will be $m \times m$ and $\Sigma$ will be $m \times n$ $0$s.

$O_{mxn} = \left[ \begin{array}{ccc} 0 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0 \end{array} \right]_{mxm} \quad \left[ \begin{array}{ccc} 0 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0 \end{array} \right]_{mxn} \left[ \begin{array}{ccc} 0 & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & 0 \end{array} \right]_{nxn}$

Intuitively a $0$ matrix scaled by $0$ in every direction so all singular values will be $0$.

\subsection*{(c) A general $x \neq 0 \in \mathbb{R}^n$ (in terms of $x$ and $\left\lVert x \right\lVert$)}

So the result of the SVD of any vector would be similar to part (a) where the $\Sigma$ singular value diagonal matrix is just going to be a $1 \times n$ matrix with $1$ singular value that will be the norm of the vector (since a vector only extends in one direction it makes sense that there's just one singular value that is the norm in that direction). Then $U$ will just be a scalar $1$ since $m$ is just $1$ and $v_1$ will be the normalized vector $\frac{x_i}{\left\lVert x \right\lVert}$ values
transposed and $\frac{A}{\sigma_1}$ will be $\frac{x_i}{\|x\|}$ vector $u_i$ will effectively be $xx^T$ which will always come out to $1$ in this case. The remaining $V$ rows will be whatever vectors are orthonormal to $v_1$ so the whole thing will look something like:

$x = \left[ 1 \right] \left[ \begin{array}{cccc} \| x \| & 0 & \cdots & 0 \end{array} \right] \left[ \begin{array}{cccc} \frac{x_1}{\|\vec{x}\|} & \cdots & \frac{x_{n-1}}{\|\vec{x}\|} & \frac{x_n}{\|\vec{x}\|} \\ v_2 \perp v_1 & \cdots & \cdots & \cdots \\ \vdots & \vdots & \ddots & \vdots \\ v_n \perp v_1 & \cdots & \cdots & \cdots \end{array} \right]$

\section{The condition number of a matrix is defined by
$ cond(G) = \bar{\sigma(G)}/\underline{\sigma}(G)$
The condition number of a matrix (or of a transfer function of a plant plotted as a function
of frequency) is a measure of the difficulty in inverting the matrix (i.e., controlling the plant).
If G and K are square, invertible complex matrices, prove the following using the submultiplicative 
property $\bar{\sigma}(GK) \leq \bar{\sigma}(G)\bar{\sigma}(K)$ of the matrix 2-norm, $\left\lVert \cdot \right\lVert_2 = \bar{\sigma}(\cdot)$ and the identity
$\bar{\sigma}(G^{-1}) = \frac{1}{\underline{\sigma}(G)}$. (Hint: $GG^{-1} = I$).
}

\subsection*{(a) $\bar{\sigma}(GK) \leq \bar{\sigma}(KG) cond(G)$}
\begin{align*}
    \text{Submultiplicative} \\
    \bar{\sigma}(GK) &\leq \bar{\sigma}(G) \bar{\sigma}(K) \\
    \bar{\sigma}(GK) &\leq \bar{\sigma}(G) \bar{\sigma}(KGG^{-1}) \\
    \bar{\sigma}(GK) &\leq \frac{\bar{\sigma}(G) \bar{\sigma}(KG)}{\underline{\sigma}(G)} \\
    \bar{\sigma}(GK) &\leq \bar{\sigma}(KG) cond(G)
\end{align*}
\subsection*{(b) $\underline{\sigma}(GK) \geq \underline{\sigma}(KG)/cond(G)$}
\begin{align*}
    \text{Submultiplicative} \\
    \bar{\sigma}(G^{-1}K^{-1}) &\leq \bar{\sigma}(G^{-1}) \bar{\sigma}(K^{-1}) \\
    \frac{1}{\underline{\sigma}(GK)} &\leq \frac{1}{\underline{\sigma}(G) \underline{\sigma}(K)} \\
    \underline{\sigma}((G) \underline{\sigma}(K)) &\leq \underline{\sigma}(GK) \\
    \underline{\sigma}((G) \underline{\sigma}(KGG^{-1})) &\leq \underline{\sigma}(GK) \\
    \frac{\underline{\sigma}(G)}{\bar{\sigma}(G)} \underline{\sigma}(KG) &\leq \underline{\sigma}(GK) \\
    \frac{\underline{\sigma}(KG)}{cond(G)} &\leq \underline{\sigma}(GK)
\end{align*}

\end{document}